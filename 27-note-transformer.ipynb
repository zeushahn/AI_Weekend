{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Transformer으로 IMDB 리뷰 분리하기\n- Transformer : Transformer는 RNN없이 순수 self_attention만으로 시퀀스를 처리하며, BERT와 같은 사전학습 언어 모델의 핵심 구조","metadata":{}},{"cell_type":"code","source":"# Library\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:13:33.943306Z","iopub.execute_input":"2025-04-06T01:13:33.943734Z","iopub.status.idle":"2025-04-06T01:13:47.696061Z","shell.execute_reply.started":"2025-04-06T01:13:33.943696Z","shell.execute_reply":"2025-04-06T01:13:47.695143Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Hyper Parameter\nnum_words = 5000\nmax_len = 100\nembedding_dim=128\nbatch_size = 64\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:13:47.697222Z","iopub.execute_input":"2025-04-06T01:13:47.697800Z","iopub.status.idle":"2025-04-06T01:13:47.774483Z","shell.execute_reply.started":"2025-04-06T01:13:47.697762Z","shell.execute_reply":"2025-04-06T01:13:47.773619Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# IMDB 데이터 로드 (5000개의 단어만 사용)\n(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=num_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:13:47.776552Z","iopub.execute_input":"2025-04-06T01:13:47.776936Z","iopub.status.idle":"2025-04-06T01:13:51.718265Z","shell.execute_reply.started":"2025-04-06T01:13:47.776912Z","shell.execute_reply":"2025-04-06T01:13:51.717359Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### 훈련데이터를 훈련과 검증으로 분할","metadata":{}},{"cell_type":"code","source":"train_input, val_input, train_target, val_target = train_test_split(\n                                                        train_input,\n                                                        train_target,\n                                                        test_size=0.2,\n                                                        random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:13:51.720183Z","iopub.execute_input":"2025-04-06T01:13:51.720417Z","iopub.status.idle":"2025-04-06T01:13:51.729550Z","shell.execute_reply.started":"2025-04-06T01:13:51.720399Z","shell.execute_reply":"2025-04-06T01:13:51.728733Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"----\n### Sequence Padding\n: 전체 자릿수를 100으로 가정했을 경우 한 문장에 3개 토큰만 있을 경우 나머지 97개는 비워지고 이를 0으로 채우는 과정","metadata":{}},{"cell_type":"code","source":"# 패딩 처리 (최대길이 100)\ntrain_seq = pad_sequences(train_input, maxlen = max_len)\nval_seq = pad_sequences(val_input, maxlen=max_len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:13:51.730409Z","iopub.execute_input":"2025-04-06T01:13:51.730623Z","iopub.status.idle":"2025-04-06T01:13:51.959956Z","shell.execute_reply.started":"2025-04-06T01:13:51.730604Z","shell.execute_reply":"2025-04-06T01:13:51.959237Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Pytorch Tensor 변환\ntrain_seq_tensor = torch.tensor(train_seq, dtype=torch.long)\ntrain_target_tensor = torch.tensor(train_target, dtype=torch.float32)\n\nval_seq_tensor = torch.tensor(val_seq, dtype=torch.long)\nval_target_tensor = torch.tensor(val_target, dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:13:51.960588Z","iopub.execute_input":"2025-04-06T01:13:51.960850Z","iopub.status.idle":"2025-04-06T01:13:51.997423Z","shell.execute_reply.started":"2025-04-06T01:13:51.960818Z","shell.execute_reply":"2025-04-06T01:13:51.996544Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Pytorch Dataloader 생성\n\ntrain_dataset = TensorDataset(train_seq_tensor, train_target_tensor)\nval_dataset = TensorDataset(val_seq_tensor, val_target_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:13:51.998327Z","iopub.execute_input":"2025-04-06T01:13:51.998620Z","iopub.status.idle":"2025-04-06T01:13:52.004834Z","shell.execute_reply.started":"2025-04-06T01:13:51.998598Z","shell.execute_reply":"2025-04-06T01:13:52.004038Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Positional Encoding(Transformer에서는 필수)\n: 순서 정보 보존 ","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n        pe = pe.unsqueeze(0) # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1), :].to(x.device)\n        return x\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:40:25.408293Z","iopub.execute_input":"2025-04-06T01:40:25.408580Z","iopub.status.idle":"2025-04-06T01:40:25.414299Z","shell.execute_reply.started":"2025-04-06T01:40:25.408558Z","shell.execute_reply":"2025-04-06T01:40:25.413436Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"### 모델 정의","metadata":{}},{"cell_type":"code","source":"class TransformerClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, nhead, num_layers, hidden_dim, output_dim, max_len=100):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.pos_encoder = PositionalEncoding(embed_dim, max_len)\n\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=hidden_dim)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        self.fc = nn.Linear(embed_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        embedded = self.embedding(x) \n        embedded = self.pos_encoder(embedded) # positional encoding\n        embedded = embedded.permute(1, 0, 2)  # Transformer (seq, batch, emb)\n        out = self.transformer_encoder(embedded)\n        out = out.mean(dim=0)\n        out = self.fc(out)\n        return self.sigmoid(out).squeeze(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:40:35.390455Z","iopub.execute_input":"2025-04-06T01:40:35.390774Z","iopub.status.idle":"2025-04-06T01:40:35.396673Z","shell.execute_reply.started":"2025-04-06T01:40:35.390748Z","shell.execute_reply":"2025-04-06T01:40:35.395832Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### 학습함수","metadata":{}},{"cell_type":"code","source":"def train(model, loader, criterion, optimizer):\n    model.train()\n    total_loss, total_correct = 0, 0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        \n        optimizer.zero_grad()\n        preds = model(x) # 예측값\n        loss = criterion(preds, y) # 손실 계산\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_correct += ((preds >= 0.5).float() == y).sum().item()\n\n    accuracy = total_correct / len(loader.dataset)\n    return total_loss / len(loader), accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:40:37.830433Z","iopub.execute_input":"2025-04-06T01:40:37.830755Z","iopub.status.idle":"2025-04-06T01:40:37.835819Z","shell.execute_reply.started":"2025-04-06T01:40:37.830716Z","shell.execute_reply":"2025-04-06T01:40:37.835021Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### 평가 함수","metadata":{}},{"cell_type":"code","source":"# 모델 평가 함수\ndef evaluate(model, loader, criterion):\n    model.eval()\n    total_loss, total_correct = 0, 0\n\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n    \n            preds = model(x) # 예측값\n            loss = criterion(preds, y) # 손실 계산\n    \n            total_loss += loss.item()\n            total_correct += ((preds >= 0.5).float() == y).sum().item()\n\n    accuracy = total_correct / len(loader.dataset)\n    return total_loss / len(loader), accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:40:40.446505Z","iopub.execute_input":"2025-04-06T01:40:40.446791Z","iopub.status.idle":"2025-04-06T01:40:40.451632Z","shell.execute_reply.started":"2025-04-06T01:40:40.446767Z","shell.execute_reply":"2025-04-06T01:40:40.450847Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### 학습 실행","metadata":{}},{"cell_type":"code","source":"# 인스턴스 정의\n\nmodel = TransformerClassifier(\n        vocab_size = num_words,\n        embed_dim = embedding_dim,\n        nhead = 4,\n        num_layers = 2,\n        hidden_dim = 256,\n        output_dim = 1,\n        max_len = max_len\n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:40:43.135897Z","iopub.execute_input":"2025-04-06T01:40:43.136190Z","iopub.status.idle":"2025-04-06T01:40:43.153637Z","shell.execute_reply.started":"2025-04-06T01:40:43.136169Z","shell.execute_reply":"2025-04-06T01:40:43.152965Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# 손실함수 및 옵티마이져 설정\ncriterion = nn.BCELoss() # Binary Cross Entropy\noptimizer = optim.Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:40:45.246412Z","iopub.execute_input":"2025-04-06T01:40:45.246722Z","iopub.status.idle":"2025-04-06T01:40:45.250856Z","shell.execute_reply.started":"2025-04-06T01:40:45.246694Z","shell.execute_reply":"2025-04-06T01:40:45.249912Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# 학습 실행\n\nnum_epochs = 20\n\nbest_val_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n    val_loss, val_acc = evaluate(model, val_loader, criterion)\n    print(f'Epoch : {epoch+1}/{num_epochs}')\n    print(f'- Train Loss : {train_loss:.4f} | {train_acc*100:.2f}%')\n    print(f'- Val Loss   : {val_loss  :.4f} | {val_acc*100:.2f}%')\n\n    # 모델 저장\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'transformer_imdb.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:40:47.168859Z","iopub.execute_input":"2025-04-06T01:40:47.169175Z","iopub.status.idle":"2025-04-06T01:42:03.996736Z","shell.execute_reply.started":"2025-04-06T01:40:47.169149Z","shell.execute_reply":"2025-04-06T01:42:03.996065Z"}},"outputs":[{"name":"stdout","text":"Epoch : 1/20\n- Train Loss : 0.6519 | 60.31%\n- Val Loss   : 0.5726 | 70.12%\nEpoch : 2/20\n- Train Loss : 0.5298 | 73.47%\n- Val Loss   : 0.5218 | 72.66%\nEpoch : 3/20\n- Train Loss : 0.4682 | 77.45%\n- Val Loss   : 0.4625 | 77.56%\nEpoch : 4/20\n- Train Loss : 0.4356 | 79.56%\n- Val Loss   : 0.4474 | 78.90%\nEpoch : 5/20\n- Train Loss : 0.4072 | 81.33%\n- Val Loss   : 0.4239 | 80.20%\nEpoch : 6/20\n- Train Loss : 0.3862 | 82.59%\n- Val Loss   : 0.4116 | 80.84%\nEpoch : 7/20\n- Train Loss : 0.3705 | 83.61%\n- Val Loss   : 0.4075 | 81.22%\nEpoch : 8/20\n- Train Loss : 0.3549 | 84.31%\n- Val Loss   : 0.4030 | 81.52%\nEpoch : 9/20\n- Train Loss : 0.3423 | 84.97%\n- Val Loss   : 0.4090 | 81.48%\nEpoch : 10/20\n- Train Loss : 0.3300 | 85.98%\n- Val Loss   : 0.3998 | 81.94%\nEpoch : 11/20\n- Train Loss : 0.3180 | 86.52%\n- Val Loss   : 0.4145 | 81.76%\nEpoch : 12/20\n- Train Loss : 0.3079 | 86.92%\n- Val Loss   : 0.4108 | 81.80%\nEpoch : 13/20\n- Train Loss : 0.2961 | 87.47%\n- Val Loss   : 0.4243 | 81.86%\nEpoch : 14/20\n- Train Loss : 0.2861 | 88.06%\n- Val Loss   : 0.4164 | 81.96%\nEpoch : 15/20\n- Train Loss : 0.2747 | 88.40%\n- Val Loss   : 0.4327 | 81.54%\nEpoch : 16/20\n- Train Loss : 0.2634 | 89.11%\n- Val Loss   : 0.4522 | 80.38%\nEpoch : 17/20\n- Train Loss : 0.2484 | 90.11%\n- Val Loss   : 0.4512 | 81.88%\nEpoch : 18/20\n- Train Loss : 0.2391 | 90.31%\n- Val Loss   : 0.4572 | 82.12%\nEpoch : 19/20\n- Train Loss : 0.2243 | 91.23%\n- Val Loss   : 0.4642 | 81.94%\nEpoch : 20/20\n- Train Loss : 0.2157 | 91.55%\n- Val Loss   : 0.4838 | 81.78%\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 모델 파라미터 설정\nvocab_size = num_words\nembedding_dim = 128\nhidden_dim = 64\noutput_dim = 1 # 감성 분류(0 or 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:13:52.036537Z","iopub.execute_input":"2025-04-06T01:13:52.036836Z","iopub.status.idle":"2025-04-06T01:13:52.049147Z","shell.execute_reply.started":"2025-04-06T01:13:52.036777Z","shell.execute_reply":"2025-04-06T01:13:52.048350Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\nmodel = AttentionLSTM(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:13:52.049833Z","iopub.execute_input":"2025-04-06T01:13:52.050073Z","iopub.status.idle":"2025-04-06T01:13:52.377994Z","shell.execute_reply.started":"2025-04-06T01:13:52.050053Z","shell.execute_reply":"2025-04-06T01:13:52.377169Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"AttentionLSTM(\n  (embedding): Embedding(5000, 128)\n  (lstm): LSTM(128, 64, batch_first=True)\n  (attn): Linear(in_features=64, out_features=1, bias=True)\n  (fc): Linear(in_features=64, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### 모델 학습 준비","metadata":{"execution":{"iopub.status.busy":"2025-04-05T06:32:03.888362Z","iopub.execute_input":"2025-04-05T06:32:03.888719Z","iopub.status.idle":"2025-04-05T06:32:03.893354Z","shell.execute_reply.started":"2025-04-05T06:32:03.888681Z","shell.execute_reply":"2025-04-05T06:32:03.892143Z"}}},{"cell_type":"code","source":"# 손실함수 및 옵티마이져 설정\ncriterion = nn.BCELoss() # Binary Cross Entropy\noptimizer = optim.Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:13:52.378704Z","iopub.execute_input":"2025-04-06T01:13:52.378957Z","iopub.status.idle":"2025-04-06T01:13:54.319707Z","shell.execute_reply.started":"2025-04-06T01:13:52.378937Z","shell.execute_reply":"2025-04-06T01:13:54.319044Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### 모델 학습 함수","metadata":{}},{"cell_type":"code","source":"def train(model, loader, criterion, optimizer):\n    model.train()\n    total_loss, total_correct = 0, 0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        \n        optimizer.zero_grad()\n        preds = model(x).squeeze(1) # 예측값\n        loss = criterion(preds, y) # 손실 계산\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_correct += ((preds >= 0.5).float() == y).sum().item()\n\n    accuracy = total_correct / len(loader.dataset)\n    return total_loss / len(loader), accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:13:54.320565Z","iopub.execute_input":"2025-04-06T01:13:54.321013Z","iopub.status.idle":"2025-04-06T01:13:54.325929Z","shell.execute_reply.started":"2025-04-06T01:13:54.320982Z","shell.execute_reply":"2025-04-06T01:13:54.325126Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# 모델 평가 함수\ndef evaluate(model, loader, criterion):\n    model.eval()\n    total_loss, total_correct = 0, 0\n\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n    \n            preds = model(x).squeeze(1) # 예측값\n            loss = criterion(preds, y) # 손실 계산\n    \n            total_loss += loss.item()\n            total_correct += ((preds >= 0.5).float() == y).sum().item()\n\n    accuracy = total_correct / len(loader.dataset)\n    return total_loss / len(loader), accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:13:54.326655Z","iopub.execute_input":"2025-04-06T01:13:54.326952Z","iopub.status.idle":"2025-04-06T01:13:54.350268Z","shell.execute_reply.started":"2025-04-06T01:13:54.326923Z","shell.execute_reply":"2025-04-06T01:13:54.349404Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### 모델 학습 실행","metadata":{}},{"cell_type":"code","source":"num_epochs = 20\n\nbest_val_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n    val_loss, val_acc = evaluate(model, val_loader, criterion)\n    print(f'Epoch : {epoch+1}/{num_epochs}')\n    print(f'- Train Loss : {train_loss:.4f} | {train_acc*100:.2f}%')\n    print(f'- Val Loss   : {val_loss  :.4f} | {val_acc*100:.2f}%')\n\n    # 모델 저장\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'best_attention_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:13:54.351214Z","iopub.execute_input":"2025-04-06T01:13:54.351556Z","iopub.status.idle":"2025-04-06T01:14:18.287670Z","shell.execute_reply.started":"2025-04-06T01:13:54.351524Z","shell.execute_reply":"2025-04-06T01:14:18.286983Z"}},"outputs":[{"name":"stdout","text":"Epoch : 1/20\n- Train Loss : 0.6866 | 57.36%\n- Val Loss   : 0.6786 | 61.06%\nEpoch : 2/20\n- Train Loss : 0.6399 | 64.93%\n- Val Loss   : 0.5993 | 68.56%\nEpoch : 3/20\n- Train Loss : 0.5615 | 71.23%\n- Val Loss   : 0.5518 | 72.00%\nEpoch : 4/20\n- Train Loss : 0.5145 | 74.84%\n- Val Loss   : 0.5176 | 74.52%\nEpoch : 5/20\n- Train Loss : 0.4820 | 77.16%\n- Val Loss   : 0.4976 | 75.68%\nEpoch : 6/20\n- Train Loss : 0.4577 | 78.94%\n- Val Loss   : 0.4835 | 76.66%\nEpoch : 7/20\n- Train Loss : 0.4365 | 79.88%\n- Val Loss   : 0.4674 | 77.84%\nEpoch : 8/20\n- Train Loss : 0.4189 | 80.83%\n- Val Loss   : 0.4579 | 78.52%\nEpoch : 9/20\n- Train Loss : 0.4023 | 81.86%\n- Val Loss   : 0.4557 | 78.74%\nEpoch : 10/20\n- Train Loss : 0.3876 | 82.81%\n- Val Loss   : 0.4479 | 79.24%\nEpoch : 11/20\n- Train Loss : 0.3748 | 83.28%\n- Val Loss   : 0.4443 | 79.12%\nEpoch : 12/20\n- Train Loss : 0.3634 | 83.94%\n- Val Loss   : 0.4371 | 79.70%\nEpoch : 13/20\n- Train Loss : 0.3505 | 84.69%\n- Val Loss   : 0.4370 | 80.44%\nEpoch : 14/20\n- Train Loss : 0.3382 | 85.35%\n- Val Loss   : 0.4320 | 80.36%\nEpoch : 15/20\n- Train Loss : 0.3278 | 85.92%\n- Val Loss   : 0.4291 | 81.08%\nEpoch : 16/20\n- Train Loss : 0.3175 | 86.48%\n- Val Loss   : 0.4289 | 80.64%\nEpoch : 17/20\n- Train Loss : 0.3063 | 87.12%\n- Val Loss   : 0.4285 | 80.92%\nEpoch : 18/20\n- Train Loss : 0.2963 | 87.67%\n- Val Loss   : 0.4237 | 81.26%\nEpoch : 19/20\n- Train Loss : 0.2858 | 88.27%\n- Val Loss   : 0.4265 | 81.14%\nEpoch : 20/20\n- Train Loss : 0.2758 | 88.77%\n- Val Loss   : 0.4439 | 81.24%\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### 모델 평가","metadata":{}},{"cell_type":"code","source":"# 저장된 모델 로드\nmodel.load_state_dict(torch.load('best_attention_model.pth', weights_only=False))\ntest_loss, test_acc = evaluate(model, val_loader, criterion)\nprint(f'Test Accuracy : {test_acc*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T01:14:18.288518Z","iopub.execute_input":"2025-04-06T01:14:18.288824Z","iopub.status.idle":"2025-04-06T01:14:18.396587Z","shell.execute_reply.started":"2025-04-06T01:14:18.288779Z","shell.execute_reply":"2025-04-06T01:14:18.395968Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy : 81.26%\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"> 전체 댓글의 토큰중 50개만 선택하여 구성하였는데 거의 70%넘게 예측력이 나왔다. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}