{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이프라인을 이용한 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-12T04:48:06.201061Z",
     "iopub.status.busy": "2025-04-12T04:48:06.200532Z",
     "iopub.status.idle": "2025-04-12T04:48:34.194231Z",
     "shell.execute_reply": "2025-04-12T04:48:34.193663Z",
     "shell.execute_reply.started": "2025-04-12T04:48:06.201036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 04:48:16.497407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744433296.745973      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744433296.844525      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:48:34.195864Z",
     "iopub.status.busy": "2025-04-12T04:48:34.195320Z",
     "iopub.status.idle": "2025-04-12T04:48:40.012369Z",
     "shell.execute_reply": "2025-04-12T04:48:40.011839Z",
     "shell.execute_reply.started": "2025-04-12T04:48:34.195845Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db45692caf24ca696d4c76d6a55fd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa5048f304d41908f7820cd68061931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74c45a0bd6242f4bbc078975b6a68b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd02f0e910a4bccae4d5a11d757a0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8bf2136b084dd7af55bf5c277d5f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7752aff3233a45aca9f23edf2669fb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d0619ac88d4cdd8668e3645ef8a737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"openai-community/gpt2\",\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu'),\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32, # GPU가 지원하는 성능 향상\n",
    "    truncation=True # 최대와 최소 토큰수 결정시 사용 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:48:51.085522Z",
     "iopub.status.busy": "2025-04-12T04:48:51.084995Z",
     "iopub.status.idle": "2025-04-12T04:48:52.741014Z",
     "shell.execute_reply": "2025-04-12T04:48:52.740199Z",
     "shell.execute_reply.started": "2025-04-12T04:48:51.085498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I am learning about tokenizers. There were two token markets that were invented a long time ago, and those exchanges worked quite well, but now they are being attacked by other exchanges who don\\'t want to acknowledge their existence,\" said Yagoda.'}]\n"
     ]
    }
   ],
   "source": [
    "inputs = \"I am learning about tokenizers.\"\n",
    "outputs = pipe(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:48:55.074836Z",
     "iopub.status.busy": "2025-04-12T04:48:55.074471Z",
     "iopub.status.idle": "2025-04-12T04:48:55.079068Z",
     "shell.execute_reply": "2025-04-12T04:48:55.078449Z",
     "shell.execute_reply.started": "2025-04-12T04:48:55.074811Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am learning about tokenizers. There were two token markets that were invented a long time ago, and those exchanges worked quite well, but now they are being attacked by other exchanges who don't want to acknowledge their existence,\" said Yagoda.\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:48:58.308934Z",
     "iopub.status.busy": "2025-04-12T04:48:58.308687Z",
     "iopub.status.idle": "2025-04-12T04:48:59.486197Z",
     "shell.execute_reply": "2025-04-12T04:48:59.485586Z",
     "shell.execute_reply.started": "2025-04-12T04:48:58.308919Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am learning about tokenizers. I am going to try it out and try with my other ideas.\n",
      "\n",
      "I wanted to give another example to illustrate how to develop systems using a smart contract to incentivize tokens, so I asked my friends over and over.\n",
      "\n",
      "\"It is a validating algorithm for the right price and it is the only way the supply will shrink.\"\n",
      "\n",
      "So far, they gave me some data on 3,000 ETHs and an average price of $1\n"
     ]
    }
   ],
   "source": [
    "inputs = \"I am learning about tokenizers.\"\n",
    "outputs = pipe(inputs, min_length=30, max_length=100) # 최소와 최대 토큰수 정하기 \n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 다른 모델 사용해보기\n",
    ":bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:50:24.587854Z",
     "iopub.status.busy": "2025-04-12T04:50:24.587130Z",
     "iopub.status.idle": "2025-04-12T04:50:27.948814Z",
     "shell.execute_reply": "2025-04-12T04:50:27.948196Z",
     "shell.execute_reply.started": "2025-04-12T04:50:24.587832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4486cd5c86974d74b93ad4fc866d92d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37dfe7c0ee26463e91d947fc36ae19f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf72ccfe92b84361af6e115ecaf2cbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fed458bd9848a5b84ed98959b30672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6190b0961d44061bbd8d2b9701455ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"google-bert/bert-base-uncased\",\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu'),\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32, # GPU가 지원하는 성능 향상\n",
    "    truncation=True # 최대와 최소 토큰수 결정시 사용 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:50:43.883866Z",
     "iopub.status.busy": "2025-04-12T04:50:43.883119Z",
     "iopub.status.idle": "2025-04-12T04:50:45.509071Z",
     "shell.execute_reply": "2025-04-12T04:50:45.508291Z",
     "shell.execute_reply.started": "2025-04-12T04:50:43.883842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am learning about tokenizers...........................................................................................\n"
     ]
    }
   ],
   "source": [
    "inputs = \"I am learning about tokenizers.\"\n",
    "outputs = pipe(inputs, min_length=30, max_length=100) # 최소와 최대 토큰수 정하기 \n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BERT 모델은 양방향 인코더 모델로 텍스트의 의미를 효과적으로 인코딩하고 이해하는데는 탁월하나 순차적으로 텍스트를 생성하는 데에는 적합하지 않다.\n",
    "> BERT는 텍스트 분류, 자연어 추론, 질의 응답, 개체명 인식등의 과제에 활용\n",
    "> 따라서 과제의 특성과 모델의 구조를 잘 파악해 적절한 모델을 선택하는 것이 중요하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 한국어 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:54:49.301873Z",
     "iopub.status.busy": "2025-04-12T04:54:49.300956Z",
     "iopub.status.idle": "2025-04-12T04:54:49.305404Z",
     "shell.execute_reply": "2025-04-12T04:54:49.304872Z",
     "shell.execute_reply.started": "2025-04-12T04:54:49.301844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:56:51.511816Z",
     "iopub.status.busy": "2025-04-12T04:56:51.511312Z",
     "iopub.status.idle": "2025-04-12T04:56:55.337160Z",
     "shell.execute_reply": "2025-04-12T04:56:55.336498Z",
     "shell.execute_reply.started": "2025-04-12T04:56:51.511792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe851fe3d25e43d892dbc9c80b9faba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73760b76964b40a68279cfad2643b3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8e201fd266445491d75feb96ca7ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5151265f5b4258becf1b99a691e331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# KoGPT2 모델과 토크나이저 불러오기\n",
    "model_name = 'skt/kogpt2-base-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:58:12.349671Z",
     "iopub.status.busy": "2025-04-12T04:58:12.349126Z",
     "iopub.status.idle": "2025-04-12T04:58:12.560298Z",
     "shell.execute_reply": "2025-04-12T04:58:12.559762Z",
     "shell.execute_reply.started": "2025-04-12T04:58:12.349622Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu'),\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32, # GPU가 지원하는 성능 향상\n",
    "    truncation=True # 최대와 최소 토큰수 결정시 사용 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T05:00:47.403489Z",
     "iopub.status.busy": "2025-04-12T05:00:47.403169Z",
     "iopub.status.idle": "2025-04-12T05:00:48.345926Z",
     "shell.execute_reply": "2025-04-12T05:00:48.345149Z",
     "shell.execute_reply.started": "2025-04-12T05:00:47.403460Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지금 비가 오는데 다들 조심하셔야겠습니다.\n",
      "어쨌든 영동과 전북동해안으로는 동풍의 영향으로 계속해서 비 조심하셔야겠습니다.\n",
      "어제가 가장 더웠던 때는 제법 강한 비구름이 내렸다고 합니다.\n",
      "어제는 비가 내리면서 그쳤는데 오늘은 다시 비가 오기 시작하면서 조금 쌀쌀하겠습니다.\n",
      "강원산간에서는 오늘부터 내일 새벽까지 최고 20mm까지 강하게 내리겠습니다.\n",
      "비는 내일 오후가 되면 대부분 그칠 것으로 보입니다.\n",
      "내일 새벽 서울경기\n"
     ]
    }
   ],
   "source": [
    "# 한글 입력 문장\n",
    "inputs = \"지금 비가 오는데\"\n",
    "outputs = pipe(inputs, min_length=30, max_length=100)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### gpt3와 비슷한 다른 모델 사용해 보기\n",
    ": gpt3는 Hugging Face에서 지원 전이고 openai의 api를 사용해야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T05:10:57.093588Z",
     "iopub.status.busy": "2025-04-12T05:10:57.092818Z",
     "iopub.status.idle": "2025-04-12T05:10:57.202501Z",
     "shell.execute_reply": "2025-04-12T05:10:57.201993Z",
     "shell.execute_reply.started": "2025-04-12T05:10:57.093563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hugging face의 mistralai/Mistral-7B-Instruct-v0.1 / \n",
    "from huggingface_hub import login\n",
    "login(token=\"___token Id ____\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T05:12:42.773144Z",
     "iopub.status.busy": "2025-04-12T05:12:42.772856Z",
     "iopub.status.idle": "2025-04-12T05:13:40.183352Z",
     "shell.execute_reply": "2025-04-12T05:13:40.182420Z",
     "shell.execute_reply.started": "2025-04-12T05:12:42.773123Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b77611788044c49caac44f3e3a885f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e69aaf628941b189074987d498fae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008f93aa24ef46b4b9ce8c58db14f173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e98f9540048431d98191768426290f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368a590b2b354893a87dc9c76e6058ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464fd12733e04547873fc7c1a51b2ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b620ad9e17649099039ecf1d1e54cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801b03e66dcc43ac813b07f95dbb1fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca35d60c3bf94caa9ca7b008c95d8a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f88b37140044cba40892dff181a661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3bcae6e76d4c5e8befca9e8237d797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토크나이저 불러오기\n",
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T05:23:12.587591Z",
     "iopub.status.busy": "2025-04-12T05:23:12.587008Z",
     "iopub.status.idle": "2025-04-12T05:23:12.593327Z",
     "shell.execute_reply": "2025-04-12T05:23:12.592752Z",
     "shell.execute_reply.started": "2025-04-12T05:23:12.587569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32, # GPU가 지원하는 성능 향상\n",
    "    truncation=True # 최대와 최소 토큰수 결정시 사용 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T05:24:08.735370Z",
     "iopub.status.busy": "2025-04-12T05:24:08.735105Z",
     "iopub.status.idle": "2025-04-12T05:24:15.755580Z",
     "shell.execute_reply": "2025-04-12T05:24:15.754994Z",
     "shell.execute_reply.started": "2025-04-12T05:24:08.735353Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning. I'm really excited to be here today, and I'm honored to have the opportunity to share my thoughts with you all.\n",
      "\n",
      "I believe that we are at a very important time in our history as a species. With the rapid pace of technological advancement, it is easy to feel overwhelmed by the changes that are happening around us. But I think it is important to remember that technology is simply a tool, and like any tool, it can be used for good or for bad\n"
     ]
    }
   ],
   "source": [
    "# 생성 테스트\n",
    "prompt = \"Good morning.\"\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature = 0.7,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 다른 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T05:27:42.463834Z",
     "iopub.status.busy": "2025-04-12T05:27:42.463106Z",
     "iopub.status.idle": "2025-04-12T05:32:34.561724Z",
     "shell.execute_reply": "2025-04-12T05:32:34.560947Z",
     "shell.execute_reply.started": "2025-04-12T05:27:42.463812Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492b5d6b944d4e26bd2fa5a4c21443bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3ac9e296454705818fd3c9f14e6644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1288dedb3546f1b053da7e54cdd94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b374a9d0e2464499a61c2b6caf9b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6184a7df4af4538861df5c3a4dc9f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1618b2a8684d198948684b5c34f666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d08a0234be04f40ad9263bd9e1f42a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/930 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec99c79410140af97aeed7318112849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 불러오기\n",
    "model_name = 'EleutherAI/gpt-j-6B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T05:35:15.298386Z",
     "iopub.status.busy": "2025-04-12T05:35:15.297716Z",
     "iopub.status.idle": "2025-04-12T05:35:19.530326Z",
     "shell.execute_reply": "2025-04-12T05:35:19.529370Z",
     "shell.execute_reply.started": "2025-04-12T05:35:15.298355Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=1,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32, # GPU가 지원하는 성능 향상\n",
    "    truncation=True # 최대와 최소 토큰수 결정시 사용 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T05:35:43.997436Z",
     "iopub.status.busy": "2025-04-12T05:35:43.996874Z",
     "iopub.status.idle": "2025-04-12T05:35:49.691811Z",
     "shell.execute_reply": "2025-04-12T05:35:49.691157Z",
     "shell.execute_reply.started": "2025-04-12T05:35:43.997413Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 오늘 아침에\n",
      "세이콘을 먹으니까\n",
      "빵을 안 타고 마시는 게\n",
      "그렇지 않도 뭘 생각하기 시작할 거야\n",
      "우\n"
     ]
    }
   ],
   "source": [
    "# 생성 테스트\n",
    "prompt = \"나는 오늘 아침에\"\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=100\n",
    ")\n",
    "\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
