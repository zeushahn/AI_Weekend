{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### KETI의 모델에 데이터를 전이학습\n- KETI(Korea Electronic Technology Institude: 한국전자기술연구원)의 모델 사용\n- 한국어 3600만개의 문서를 영어에 대해서 3800만개의 문서를 학습한 데이터\n- 데이터세트는 HelsinkiNLP에서 공개한 OPUS-100을 사용해 영어를 한국어로 번역\n- OPUS-100 데이터세트는 전 세계 100개 언어와 언어의 쌍으로 이루어진 데이터세트로 약 5500만개의 문장으로 구성","metadata":{}},{"cell_type":"markdown","source":"### OPUS-100 데이터 세트 토큰화","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import T5TokenizerFast, T5ForConditionalGeneration","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:21:56.486367Z","iopub.execute_input":"2025-04-13T06:21:56.486914Z","iopub.status.idle":"2025-04-13T06:22:21.120993Z","shell.execute_reply.started":"2025-04-13T06:21:56.486890Z","shell.execute_reply":"2025-04-13T06:22:21.120447Z"}},"outputs":[{"name":"stderr","text":"2025-04-13 06:22:10.683464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744525330.875281      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744525330.928622      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"model_name = \"KETI-AIR/long-ke-t5-small\"\ntokenizer = T5TokenizerFast.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:23:48.814290Z","iopub.execute_input":"2025-04-13T06:23:48.815272Z","iopub.status.idle":"2025-04-13T06:23:52.431290Z","shell.execute_reply.started":"2025-04-13T06:23:48.815246Z","shell.execute_reply":"2025-04-13T06:23:52.430791Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92c5117b87604d4c8ef2a8f37156285c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.59M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5674e372732544269f3e6bb22a8c614d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.17M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6e9153afb4f44fdba11d5063a64086a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed05aa0ffa3447939998f4944b790448"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/893 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cd449ff567b4df19c6e76b9e6e55c86"}},"metadata":{}},{"name":"stderr","text":"You are using a model of type longt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/439M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7aa35ea56ae463f915b6e80fb534586"}},"metadata":{}},{"name":"stderr","text":"Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at KETI-AIR/long-ke-t5-small and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ko\") # 영어-한국어 부분은 100만개의 학습데이터와 2000개의 검증 데이터로 구성","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:25:15.333265Z","iopub.execute_input":"2025-04-13T06:25:15.333837Z","iopub.status.idle":"2025-04-13T06:25:20.500790Z","shell.execute_reply.started":"2025-04-13T06:25:15.333814Z","shell.execute_reply":"2025-04-13T06:25:20.500072Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/65.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c0e41a94af841ad91aacabd0ec8ea6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/143k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdcaf47bc7dc4b2eb08a2799dfdc6994"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/70.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"772b8cbb7b794fc193a24dfd7ad58108"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/144k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7bc0bc27f48410b8de875d353cc214c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15926f95c6ff4b9d915f471a37362a58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f084804d13a4eff8a88fbc54b088ed0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96f73f08dc654b0388cafcbab9686946"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def preprocess_data(example, tokenizer):\n    translation = example['translation']\n    translation_source = ['en: ' + instance['en'] for instance in translation]\n    translation_target = ['ko: ' + instance['ko'] for instance in translation]\n    tokenized = tokenizer(\n        translation_source,\n        text_target=translation_target,\n        truncation = True\n    )\n    return tokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:40:54.689590Z","iopub.execute_input":"2025-04-13T06:40:54.689875Z","iopub.status.idle":"2025-04-13T06:40:54.694333Z","shell.execute_reply.started":"2025-04-13T06:40:54.689855Z","shell.execute_reply":"2025-04-13T06:40:54.693529Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"processed_dataset = dataset.map(\n    lambda example: preprocess_data(example, tokenizer),\n    batched=True,\n    remove_columns = dataset['train'].column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:42:43.765469Z","iopub.execute_input":"2025-04-13T06:42:43.766303Z","iopub.status.idle":"2025-04-13T06:43:50.022827Z","shell.execute_reply.started":"2025-04-13T06:42:43.766272Z","shell.execute_reply":"2025-04-13T06:43:50.022239Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efe556eff38b4ce9a671ad75a180992c"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e00adf8a03234b96b80c4b2617d029ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d9a49f643d342d0bba22ead3aed6c51"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"sample = processed_dataset['test'][0]\nprint(sample)\nprint(\"영어:\", tokenizer.decode(sample['input_ids']))\nprint(\"한글:\", tokenizer.decode(sample['labels']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:45:55.821816Z","iopub.execute_input":"2025-04-13T06:45:55.822574Z","iopub.status.idle":"2025-04-13T06:45:55.827937Z","shell.execute_reply.started":"2025-04-13T06:45:55.822544Z","shell.execute_reply":"2025-04-13T06:45:55.827223Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [20004, 20525, 20048, 20298, 20480, 20025, 20263, 20027, 20187, 20050, 43305, 20009, 21015, 20047, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [20004, 23477, 20048, 92, 14, 4256, 11, 1363, 71, 1133, 2951, 20371, 33, 16, 75, 242, 10, 513, 20047, 1]}\n영어: en: What makes you think I want an intro to anyone?</s>\n한글: ko: 내가 너를 누구에게 소개하고 싶어한다고 생각하니?</s>\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### 기계 번역 모델 학습","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:47:22.330428Z","iopub.execute_input":"2025-04-13T06:47:22.331038Z","iopub.status.idle":"2025-04-13T06:47:23.890199Z","shell.execute_reply.started":"2025-04-13T06:47:22.331016Z","shell.execute_reply":"2025-04-13T06:47:23.889638Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"seq2seq_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    padding = 'longest',\n    return_tensors='pt'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:48:21.039594Z","iopub.execute_input":"2025-04-13T06:48:21.039846Z","iopub.status.idle":"2025-04-13T06:48:21.043285Z","shell.execute_reply.started":"2025-04-13T06:48:21.039829Z","shell.execute_reply":"2025-04-13T06:48:21.042735Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Hyper Parameter\ntraining_arguments = Seq2SeqTrainingArguments(\n    output_dir = \".\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=5e-5,\n    num_train_epochs=1,\n    eval_steps = 2000,\n    logging_steps = 2000,\n    seed=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:50:57.532276Z","iopub.execute_input":"2025-04-13T06:50:57.532791Z","iopub.status.idle":"2025-04-13T06:50:57.569072Z","shell.execute_reply.started":"2025-04-13T06:50:57.532759Z","shell.execute_reply":"2025-04-13T06:50:57.568557Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_arguments,\n    data_collator=seq2seq_collator,\n    train_dataset=processed_dataset['train'].select(range(100000)),\n    eval_dataset=processed_dataset['validation'].select(range(1000))\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:52:57.293222Z","iopub.execute_input":"2025-04-13T06:52:57.293537Z","iopub.status.idle":"2025-04-13T06:53:00.289766Z","shell.execute_reply.started":"2025-04-13T06:52:57.293497Z","shell.execute_reply":"2025-04-13T06:53:00.289188Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import wandb, os\nwandb.login(key=\"349fe2034aca280a50c69ff319105cf8df84cc34\")\nos.environ['WANDB_CONSOLE'] ='wrap'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:53:42.199898Z","iopub.execute_input":"2025-04-13T06:53:42.200413Z","iopub.status.idle":"2025-04-13T06:53:47.845487Z","shell.execute_reply.started":"2025-04-13T06:53:42.200388Z","shell.execute_reply":"2025-04-13T06:53:47.844845Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzeushahn\u001b[0m (\u001b[33mzeushahn-khankong\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:53:57.488020Z","iopub.execute_input":"2025-04-13T06:53:57.488679Z","iopub.status.idle":"2025-04-13T07:17:52.705315Z","shell.execute_reply.started":"2025-04-13T06:53:57.488655Z","shell.execute_reply":"2025-04-13T07:17:52.704681Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250413_065357-1exzaeli</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/zeushahn-khankong/huggingface/runs/1exzaeli' target=\"_blank\">.</a></strong> to <a href='https://wandb.ai/zeushahn-khankong/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/zeushahn-khankong/huggingface' target=\"_blank\">https://wandb.ai/zeushahn-khankong/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/zeushahn-khankong/huggingface/runs/1exzaeli' target=\"_blank\">https://wandb.ai/zeushahn-khankong/huggingface/runs/1exzaeli</a>"},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6250/6250 23:45, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2000</td>\n      <td>3.111600</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>2.874400</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>2.813100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6250, training_loss=2.92708841796875, metrics={'train_runtime': 1434.7098, 'train_samples_per_second': 69.701, 'train_steps_per_second': 4.356, 'total_flos': 1818984778039296.0, 'train_loss': 2.92708841796875, 'epoch': 1.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"### 기계 번역 수행","metadata":{}},{"cell_type":"code","source":"import torch\n\nmodel.eval()\ndevice=torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:19:37.535058Z","iopub.execute_input":"2025-04-13T07:19:37.535678Z","iopub.status.idle":"2025-04-13T07:19:37.549841Z","shell.execute_reply.started":"2025-04-13T07:19:37.535657Z","shell.execute_reply":"2025-04-13T07:19:37.549079Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(64100, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(64100, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(64100, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=64100, bias=False)\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"data = \"en: It's alaways great to acquire new knowledge.\"\ninputs = tokenizer(data, return_tensors='pt').to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:24:54.607238Z","iopub.execute_input":"2025-04-13T07:24:54.607531Z","iopub.status.idle":"2025-04-13T07:24:54.613235Z","shell.execute_reply.started":"2025-04-13T07:24:54.607490Z","shell.execute_reply":"2025-04-13T07:24:54.612433Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"with torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_length=512,\n        num_beams=4,\n        no_repeat_ngram_size=2,\n        early_stopping=False\n    )\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:25:13.689336Z","iopub.execute_input":"2025-04-13T07:25:13.689625Z","iopub.status.idle":"2025-04-13T07:25:13.936058Z","shell.execute_reply.started":"2025-04-13T07:25:13.689606Z","shell.execute_reply":"2025-04-13T07:25:13.935289Z"}},"outputs":[{"name":"stdout","text":"ko: 새로운 지식을 얻을 수 있어\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}